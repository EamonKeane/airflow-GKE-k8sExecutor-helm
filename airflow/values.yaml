# Airflow RBAC service account is installed here, 
# so ensure that this is changed if using a different namespace
namespace: airflow

testEnvironment: true

nfs:
  createServer: true
  PVCName: nfs-pvc
  PVCSize: 100Gi

google: 
  enabled: false
  installPostgresService: true
  installCloudsqlProxyDeploy: true
  project: 
  region: europe-west2
  databaseInstance: airflow
  databaseName: airflow

azure:
  enabled: false
  location: 
  azureFileStorageClassName: azurefile-airflow
  createAzureFileStorageClass: true
  createLogPVC: true
  createDagPVC: true
  storageAccountName: squarerouteairflow

local:
  enabled: false
  createLogPVC: true
  createDagPVC: true
  dagFolder: /tmp/airflowDags
  logFolder: /tmp/airflowLogs

createWorkerRBAC: true

logVolume:
  # e.g dags stored at /exports/dags
  logSubPath: logs
  # This must be the same of smaller than the size of the PV you created
  size: 10Gi
  installPV: true
  installPVC: true

dagVolume:
  # e.g dags stored at /exports/dags
  dagSubPath: dags
  # This must be the same of smaller than the size of the PV you created
  size: 10Gi
  installPV: true
  installPVC: true
  

#### If not using the install script, create the secret below using the following form:
# kubectl create secret generic airflow \
#     --from-literal=fernet-key=$FERNET_KEY \
#     --from-literal=sql_alchemy_conn=$SQL_ALCHEMY_CONN \

secrets:
  name: airflow
  key:
    # Generate from the cryptography package
    # FERNET_KEY=$(python -c "from cryptography.fernet import Fernet; print(Fernet.generate_key().decode())")
    fernetKey: fernet-key
    # In the format: 
    # postgresql+psycopg2://$POSTGRES_USER:$POSTGRES_PASSWORD@$POSTGRES_HOST:$POSTGRES_PORT/$POSTGRES_DB
    sqlAlchemyConn: sql_alchemy_conn

airflowCfg:
  core:
    dagsFolder: /usr/local/airflow/dags
    remoteLogging: False
    remoteLogConnId: 
    executor: KubernetesExecutor
    parallelism: 32
    dagConcurrency: 16
  scheduler:
    schedulerHeartbeatSec: 10
    maxThreads: 2
    serviceAccountName: airflow-rbac
  kubernetes:
    workerServiceAccountName: airflow-rbac
    deleteWorkerPods: True
    # This should be the same as the webScheduler.airflowCfgConfigMap unless 
    # you are creating a second configmap. 
    # There is currently only one templated (airflow-cfg-configmap.yaml). 
    # This is named {{ .Values.webScheduler.airflowCfgConfigMap }}
    airflowConfigmap: airflow-configmap
    workerContainerRepository: quay.io/logistio/airflow
    workerContainerTag: "develop_latest"
    namespace: airflow
    # Add a list of secrets in the form of ENV_VARIABLE=KUBERNETES_SECRET_NAME=KUBERNETES_SECRET_KEY
    # This is injected into the environment of the Kubernetes Executor workers
  kubernetesSecrets: |-
    SQL_ALCHEMY_CONN=airflow=sql_alchemy_conn
    AIRFLOW__CORE__FERNET_KEY=airflow=fernet-key
    # If using remote logging, enable this key to inject the log folder into the worker environment
    # AIRFLOW__CORE__REMOTE_BASE_LOG_FOLDER=airflow=gcs-log-folder
    # Add a list of key value pairs for node labels you want the Kubernetes Exeuctor workers
    # To be scheduled on. You can do this by using the kubectl label nodes
    # https://kubernetes.io/docs/concepts/configuration/assign-pod-node/
  # If you have created a second pool for workers, put the node labels here
  kubernetesNodeSelectors: |-
    # airflow=airflow_workers
    # pool=preemptible


# This section contains values for the pre-install hook, the web and the scheduler deployments
webScheduler:
  # Ensure the label is applied to the leader nodes (kubectl label airflow-leader)
  # $NODE_NAME=gke-airflow-default-pool-e573c3f7-nhx4
  # e.g.kubectl label node $NODE_NAME app=airflow-leader
  nodeSelector:
    # app: airflow-leader
  web:
    name: web
    resources:
      limits:
        cpu: 500m
        memory: 1Gi
      requests:
        cpu: 500m
        memory: 1Gi
    # If setting authenticate to true, create a secret of the form:
    # kubectl create secret generic google-oauth \
    #     --from-literal=client_id=$CLIENT_ID \
    #     --from-literal=client_secret=$CLIENT_SECRET 
    authenticate: False
    ## These values are only needed if authenticate is set to true
    authBackend: airflow.contrib.auth.backends.google_auth
    googleAuthDomain: mysite.io
    googleAuthSecret: google-oauth
    googleAuthSecretClientIDKey: client_id
    googleAuthSecretClientSecretKey: client_secret
    ##
    dagVolumeReadOnly: False
    replicaCount: 1
    service:
      type: ClusterIP
      port: 8080
  scheduler:
    name: scheduler
    resources:
      limits:
        cpu: 500m
        memory: 1Gi
      requests:
        cpu: 500m
        memory: 1Gi
    dagVolumeReadOnly: False
    serviceAccountName: airflow-rbac
    # kubeconfigMountPath: /root/airflow/kubeconfig
    catchupByDefault: False
  preHook:
    # This performs the migrations and runs before any new deployment or upgrade
    install: true
    initContainer: true
    args: "airflow upgradedb && alembic upgrade heads"
  installWebServer: true
  installScheduler: true
  image: quay.io/logistio/airflow
  tag: "develop_latest"
  imagePullPolicy: IfNotPresent
  dagsVolumeClaim: airflow-dags
  logsVolumeClaim: airflow-logs
  dagsMountPath: /usr/local/airflow/dags
  logsMountPath: /usr/local/airflow/logs
  dagsVolumeClaimSubPath:
  airflowCfgPath: /usr/local/airflow/airflow.cfg
  airflowCfgConfigMap: airflow-configmap

ingress:
  enabled: false
  annotations:
    # Install nginx-ingress to have this annotation create an ingress
    # https://hub.kubeapps.com/charts/stable/nginx-ingress
    kubernetes.io/ingress.class: "nginx"
    kubernetes.io/tls-acme: "true"
    # Use cert-manager to automatically provision acme certs
    # https://hub.kubeapps.com/charts/stable/cert-manager
    # This ensures that a user gets directed to the same webserver pod if using multiple
    # webserver pods during a session.
    # https://github.com/kubernetes/ingress-nginx/tree/master/docs/examples/affinity/cookie
    nginx.ingress.kubernetes.io/affinity: "cookie"
    nginx.ingress.kubernetes.io/session-cookie-name: "route"
    nginx.ingress.kubernetes.io/session-cookie-hash: "sha1"
  path: ""
  hosts: 
    - airflow.mysite.io
  tls:
  - hosts:
    - airflow.mysite.io
    secretName: airflow.mysite.io
